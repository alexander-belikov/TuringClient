{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kotta Demo V2.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import the Kotta module\n",
    "from kotta import Kotta, KottaJob\n",
    "from kotta.kotta_functions import *\n",
    "\n",
    "# Create a Kotta Connection using Login with Amazon credentials\n",
    "# The token from Kotta is stored in the auth.file\n",
    "konn = Kotta(open('../auth.file').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Round 1. Extract yearly info from the contributors table.\n",
    "\n",
    "This gets you (publication_id, author_id, full_name, year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "extract_script = '''#!/bin/bash\n",
    "apt-get -y install mysql-client-5.6;\n",
    "YEAR=$1\n",
    "mysql -h wos2.cvirc91pe37a.us-east-1.rds.amazonaws.com -P 3306 -u $wosuser -p$wospasswd -e \"use wos; SELECT wos_id, cluster_id, full_name FROM $1_contributors \n",
    "WHERE cluster_id NOT LIKE 'NULL' ;\" > $1_extract.tsv '''\n",
    "\n",
    "\n",
    "extract_2011 = KottaJob( jobtype            = 'script',                      \n",
    "                         jobname            = 'Extract 2011',                                   \n",
    "                         outputs            = ['2011_extract.tsv'],                        \n",
    "                         executable         = '/bin/bash myscript.sh 2011',\n",
    "                         script_name        = 'myscript.sh',                         \n",
    "                         script             =  extract_script\n",
    "                       )\n",
    "extract_2012 = KottaJob( jobtype            = 'script',                      \n",
    "                         jobname            = 'Extract 2012',                                   \n",
    "                         outputs            = ['2012_extract.tsv'],                                \n",
    "                         executable         = '/bin/bash myscript.sh 2012',\n",
    "                         script_name        = 'myscript.sh',                         \n",
    "                         script             =  extract_script\n",
    "                       )\n",
    "extract_2013 = KottaJob( jobtype            = 'script',                      \n",
    "                         jobname            = 'Extract 2013',                                   \n",
    "                         outputs            = ['2013_extract.tsv'],       \n",
    "                         executable         = '/bin/bash myscript.sh 2013',\n",
    "                         script_name        = 'myscript.sh',                         \n",
    "                         script             =  extract_script\n",
    "                       )\n",
    "extract_2014 = KottaJob( jobtype            = 'script',                      \n",
    "                         jobname            = 'Extract 2014',                                   \n",
    "                         outputs            = ['2014_extract.tsv'],       \n",
    "                         executable         = '/bin/bash myscript.sh 2014',\n",
    "                         script_name        = 'myscript.sh',                         \n",
    "                         script             =  extract_script\n",
    "                       )\n",
    "extract_2015 = KottaJob( jobtype            = 'script',                      \n",
    "                         jobname            = 'Extract 2015',                                   \n",
    "                         outputs            = ['2015_extract.tsv'],       \n",
    "                         executable         = '/bin/bash myscript.sh 2015',\n",
    "                         script_name        = 'myscript.sh',                         \n",
    "                         script             =  extract_script\n",
    "                       )\n",
    "\n",
    "# Now to manage these jobs better, let's put them in a list\n",
    "round1_jobs = [extract_2011, extract_2012, extract_2013, extract_2014, extract_2015]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's run these extraction routines on Kotta\n",
    "for job in round1_jobs:\n",
    "    job.submit(konn)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# If the tasks we submit are short tasks, we can easily check for the status\n",
    "# by calling the JOB.status(<Kotta_conn>) \n",
    "for job in round1_jobs:\n",
    "    print('{0} {1}'.format(job.job_id, job.status(konn)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Or more conveniently we have a wait function\n",
    "[job.wait(konn) for job in round1_jobs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for job in round1_jobs:\n",
    "    print(job.outputs[0].s3_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Round 2. Merge and Analyze the data from [2011..2015]\n",
    "\n",
    "We now have the data from the year 2011...2015, which is a lot of data. So we trim this down by aggregating\n",
    "and selecting just the top authors. Python does these sorts of tasks very well. Let's see how to do python\n",
    "on Kotta directly from a Jupyter Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "round2_inputs = [job.outputs[1].s3_url for job in round1_jobs]\n",
    "print(round2_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inputs to the Kotta and Arguments passed to the python function\n",
    "\n",
    "Inputs to the @kottajob decorator are files that show up in the working directory of the analysis.\n",
    "Arguments passed to the python function are serialized and show up on the execution side.\n",
    "Note that we are passing the outputs from round1 (Extraction) as inputs to round2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#%%time\n",
    "\n",
    "@kottajob(konn, 'Test', 10, block=True) #inputs=round2_inputs, block=True)\n",
    "def list_cwd():\n",
    "    import os;\n",
    "    return os.listdir('.')\n",
    "\n",
    "result = list_cwd()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result = list_cwd()(inputs=[round2_inputs[0:1])\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@kottajob(konn, 'Prod', 60, inputs=round2_inputs, outputs=['sorted.pkl'], block=True)\n",
    "def Merge_and_Analyze (datadir='.', top_N=10, outfile='sorted.pkl'):\n",
    "    import os\n",
    "    import csv\n",
    "    import pickle\n",
    "\n",
    "    sources    = [f for f in os.listdir(datadir) if f.endswith('_extract.tsv')]\n",
    "    delimiter  = '\\t'\n",
    "    data       = []\n",
    "    for source in sources:\n",
    "        year = int(source[0:4])\n",
    "        with open(source, 'r') as f:\n",
    "            headers = f.readline().strip().split(delimiter)\n",
    "            reader = csv.DictReader(f, delimiter=delimiter, fieldnames=headers)\n",
    "            current = list(reader)\n",
    "            for row in current:\n",
    "                row['year'] = year\n",
    "            data.extend(current)\n",
    "\n",
    "    print(\"Analyzing {0} entries\".format(len(data)))\n",
    "\n",
    "    # Do counting                                                                                                                                                                                                    \n",
    "    counts = {}\n",
    "    for row in data:\n",
    "        cid = row['cluster_id']\n",
    "        if row['cluster_id'] not in counts :\n",
    "            counts[cid] = {'data' : row, 'count' : 0}\n",
    "        counts[cid]['count'] += 1\n",
    "\n",
    "\n",
    "    # Print unique clusters                                                                                                                                                                                          \n",
    "    print(\"Unique cluster_ids : \", len(counts.keys()))\n",
    "\n",
    "    # Sort by authors based on total publications                                                                                                                                                                    \n",
    "    sorted_auths = sorted(counts, key=lambda k : counts[k]['count'])\n",
    "\n",
    "    results = {}\n",
    "    for cid in sorted_auths[-(top_N):]:\n",
    "        pubs = [{'wos_id' : row['wos_id'],\n",
    "                 'year'   : row['year'] } for row in data if row['cluster_id'] == cid]\n",
    "        results[cid] = {'pubs' :  pubs,\n",
    "                           'info' : counts[cid]\n",
    "                          }\n",
    "\n",
    "    with open(outfile, 'wb') as outf:\n",
    "        pickle.dump(results, outf)\n",
    "    return results\n",
    "\n",
    "\n",
    "merged_results = Merge_and_Analyze(datadir='.', top_N=10, outfile='sorted.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for author_id in merged_results:\n",
    "    print(\"AuthorID:{0:10} Name:{1:20} PubCount:{2:5}\".format(author_id,\n",
    "                                                      merged_results[author_id]['info']['data']['full_name'],\n",
    "                                                      merged_results[author_id]['info']['count']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Round 3 \n",
    "\n",
    "Now for the top N authors, we have their publications, we have to find the funding sources associated with each of their publications.\n",
    "This involves a time consuming query operation that can be easily parallelized. We give each author's information to different Kotta Job which query the database in parallel.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@kottajob(konn, 'Test', 10, block=True)\n",
    "def get_funding_for_author(data, outfile='final.pkl', wosuser=None, wospasswd=None):\n",
    "\n",
    "    import os\n",
    "    import pymysql.cursors\n",
    "    import pickle\n",
    "    import copy\n",
    "\n",
    "    wos_user   = wosuser   if wosuser   else os.environ['wosuser']\n",
    "    wos_passwd = wospasswd if wospasswd else os.environ['wospasswd']\n",
    "          \n",
    "    connection = pymysql.connect(host        = 'wos2.cvirc91pe37a.us-east-1.rds.amazonaws.com',\n",
    "                                 user        = wos_user,\n",
    "                                 password    = wos_passwd,\n",
    "                                 db          = 'wos',\n",
    "                                 charset     = 'utf8mb4',\n",
    "                                 cursorclass = pymysql.cursors.DictCursor)\n",
    "\n",
    "    try:\n",
    "        with connection.cursor() as cursor:\n",
    "            for author in data.keys():\n",
    "                print(\"Getting author : {0} items:{1}\".format(author, len(data[author]['pubs'])))\n",
    "                count = 0\n",
    "                for pubs in data[author]['pubs'][:50]:\n",
    "                    sql = \"select agency from {0}_funding where wos_id = '{1}' ;\".format(pubs['year'], \n",
    "                                                                                         pubs['wos_id'])                                                                                                                                                                                            \n",
    "                    cursor.execute(sql)   \n",
    "                    pubs['agency'] = cursor.fetchall()\n",
    "                    print(\"Item : {0} \", count); count += 1\n",
    "    finally:\n",
    "        connection.close()\n",
    "\n",
    "    #pickle.dump(data, open(outfile, 'wb'))                                                                                                                                                                          \n",
    "    print(\"Done\")\n",
    "    return data\n",
    "\n",
    "\n",
    "funding_data = get_funding_for_author(merged_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Checking sanity of results\n",
    "print(len(funding_data['3286000']['pubs']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Round 4 \n",
    "\n",
    "Vizualize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import operator\n",
    "\n",
    "''' Kotta Author Data\n",
    "'''\n",
    "\n",
    "# In case the SQL query takes too long, use the Pickled results\n",
    "\n",
    "'''\n",
    "pkl_fp  = open('/home/yadu/TuringClient/demo/final.pkl', 'rb')\n",
    "raw_author_agencies = pkl.load(pkl_fp)\n",
    "pkl_fp.close()\n",
    "'''\n",
    "raw_author_agencies  = funding_data\n",
    "\n",
    "#print(raw_author_agencies['1218500'])\n",
    "''' Graph initialization\n",
    "'''\n",
    "G = nx.Graph()\n",
    "\n",
    "colormap = np.array([\n",
    "    \"#1f77b4\", \"#aec7e8\", \"#ff7f0e\", \"#ffbb78\", \"#2ca02c\", \n",
    "    \"#98df8a\", \"#d62728\", \"#ff9896\", \"#9467bd\", \"#ffff00\"])\n",
    "\n",
    "\n",
    "year_mapping = {2010: colormap[5],\n",
    "                2011: colormap[1], \n",
    "                2012: colormap[2],\n",
    "                2013: colormap[3],\n",
    "                2014: colormap[4],\n",
    "                2015: colormap[0]}\n",
    "\n",
    "''' Find the authors, and gather the frequencies to find the most frequently appearing agencies\n",
    "'''\n",
    "authors = [author_id for author_id in raw_author_agencies]\n",
    "agencies = []\n",
    "freqs = {}\n",
    "for author_id in raw_author_agencies:\n",
    "    info = raw_author_agencies[author_id]\n",
    "    for pub in info['pubs']:\n",
    "        for agency in pub.get('agency', []):\n",
    "            agencies += [agency['agency']]\n",
    "            if agency['agency'] in freqs:\n",
    "                freqs[agency['agency']] += 1\n",
    "            else:\n",
    "                freqs[agency['agency']] = 1\n",
    "agencies = [agency[0] for agency in sorted(freqs.items(), key=operator.itemgetter(1), reverse=True)[:10]]\n",
    "\n",
    "''' Build the edgelist for the graph\n",
    "'''\n",
    "author_agencies = []\n",
    "for author_id in raw_author_agencies:\n",
    "    info = raw_author_agencies[author_id]\n",
    "    for pub in info['pubs']:\n",
    "        for agency in pub.get('agency', []):\n",
    "            if agency['agency'] in agencies:\n",
    "                author_agencies += [(author_id, agency['agency'], {'year':pub['year']})]\n",
    "author_agencies= sorted(author_agencies, key=lambda x: x[2]['year'], reverse=True) # sort by decreasing year, to make sure the edge gets colored by the earliest year\n",
    "\n",
    "''' Compute the edge weights (number of times an author & agency appear)\n",
    "'''\n",
    "weights = {(edge[0], edge[1]): len([uv for uv in author_agencies if (uv[0] == edge[0] and uv[1] == edge[1]) \n",
    "                                                  or (uv[1] == edge[0] and uv[0] == edge[1])]) for edge in author_agencies}\n",
    "      \n",
    "''' Add the nodes & edges to the graph\n",
    "'''\n",
    "G.add_nodes_from(authors)\n",
    "G.add_nodes_from(agencies)\n",
    "G.add_edges_from(author_agencies)\n",
    "\n",
    "''' Assign the weights to the edges, and scale accordingly\n",
    "'''\n",
    "max_weight = max([weights[x] for x in weights])\n",
    "for weight in weights:\n",
    "    G[weight[0]][weight[1]]['width'] =  8. * (weights[weight] / max_weight)# / min_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "pos = nx.spring_layout(G, weight=None)\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(10,10))\n",
    "\n",
    "'''Add the year-color mapping legend\n",
    "'''\n",
    "patches = []\n",
    "for year in year_mapping:\n",
    "    patch = mpatches.Patch(color=year_mapping[year], label=year)\n",
    "    patches += [patch]\n",
    "\n",
    "''' Assign the color nodes -- Green to agency, Red to author\n",
    "'''\n",
    "agency_color = ['#00FF00'] * len(agencies)\n",
    "author_color = ['#FF0000'] * len(authors)\n",
    "max_degree = max([degree for degree in G.degree(sorted(agencies)).values()])\n",
    "\n",
    "''' Draw agencies\n",
    "    - Scale size by degree\n",
    "'''\n",
    "nx.draw_networkx_nodes(G, pos=pos, ax=ax, linewidths=0, \n",
    "                       node_size=[1500*float(degree/max_degree) for degree in G.degree(sorted(agencies)).values()], \n",
    "                       nodelist=sorted(agencies), \n",
    "                       node_color=agency_color, \n",
    "                       node_shape='^')\n",
    "\n",
    "''' Draw authors\n",
    "'''\n",
    "nx.draw_networkx_nodes(G, pos=pos, ax=ax, linewidths=0,nodelist=authors, node_color=author_color)\n",
    "\n",
    "''' Draw edges\n",
    "    - Scale width of edge by precomputed weight\n",
    "    - Color edge by earliest year\n",
    "'''\n",
    "nx.draw_networkx_edges(G, pos=pos, ax=ax, \n",
    "                       width=[G[edge[0]][edge[1]]['width'] for edge in G.edges_iter()], \n",
    "                       edge_color=[year_mapping[G[edge[0]][edge[1]]['year']] for edge in G.edges_iter()],\n",
    "                       alpha=0.75)\n",
    "''' Draw agency names\n",
    "'''\n",
    "nx.draw_networkx_labels(G, pos=pos, ax=ax, labels={agency_name:agency_name for agency_name in agencies})\n",
    "\n",
    "plt.legend(handles=patches, bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "N = len(agencies)\n",
    "degree_dist = G.degree(sorted(agencies)).values()\n",
    "\n",
    "ind = np.arange(N)  \n",
    "width = 0.35       \n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "rects1 = ax.bar(ind, degree_dist, width, color='r')\n",
    "\n",
    "ax.set_ylabel('Degree')\n",
    "ax.set_title('Agency Degree')\n",
    "ax.set_xticks(ind + width / 2)\n",
    "ax.set_xticklabels(G.degree(sorted(agencies)).keys(), rotation='vertical')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
